import os
import json
from typing import List, Optional, Dict
import faiss
import numpy as np
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from src.image_processor import ImageProcessor

# Set environment variable
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


class APIServer:
    """FastAPI server for multimodal search"""

    def __init__(
        self,
        index_file: str = "output/faiss_visual.index",
        mapping_file: str = "output/index_to_path.json",
        transcript_dir: str = "output/transcripts",
        model_id: str = "openai/clip-vit-base-patch32",
    ):
        """
        Initialize API server

        Args:
            index_file: Path to FAISS index file
            mapping_file: Path to index mapping JSON file
            transcript_dir: Directory containing transcript files
            model_id: CLIP model identifier
        """
        self.index_file = index_file
        self.mapping_file = mapping_file
        self.transcript_dir = transcript_dir  # L∆∞u l·∫°i ƒë∆∞·ªùng d·∫´n

        # Initialize processors
        print("Initializing image processor...")
        self.image_processor = ImageProcessor(model_id)

        # Load FAISS index and mapping
        print("Loading FAISS index and mapping...")
        self.index = faiss.read_index(index_file)
        with open(mapping_file, "r") as f:
            self.index_to_path = json.load(f)
        print(f"‚úÖ Loaded index with {self.index.ntotal} vectors")

        # T·∫£i t·∫•t c·∫£ c√°c transcript v√†o b·ªô nh·ªõ
        print("Loading audio transcripts...")
        self.transcripts = self._load_transcripts()
        print(f"‚úÖ Loaded {len(self.transcripts)} transcripts.")

        # Initialize FastAPI app
        self.app = self._create_app()

    # H√†m ƒë·ªÉ t·∫£i transcript
    def _load_transcripts(self) -> Dict[str, List[Dict]]:
        """Load all JSON transcripts from the output directory."""
        transcripts = {}
        if not os.path.exists(self.transcript_dir):
            print(f"‚ö†Ô∏è Transcript directory not found: {self.transcript_dir}")
            return transcripts

        for filename in os.listdir(self.transcript_dir):
            if filename.endswith(".json"):
                # L·∫•y t√™n file media g·ªëc (vd: video999.json -> video999)
                media_name = os.path.splitext(filename)[0]

                # T√¨m ƒë∆∞·ªùng d·∫´n file media ƒë·∫ßy ƒë·ªß trong mapping
                # ƒêi·ªÅu n√†y gi·∫£ ƒë·ªãnh file media (mp4) c≈©ng c√≥ trong data
                # v√† ƒë√£ ƒë∆∞·ª£c index (√≠t nh·∫•t 1 frame) trong faiss_visual.index
                original_path = None
                for path in self.index_to_path.values():
                    if os.path.splitext(os.path.basename(path))[0] == media_name:
                        original_path = os.path.basename(path)
                        break

                if original_path:
                    try:
                        with open(
                            os.path.join(self.transcript_dir, filename),
                            "r",
                            encoding="utf-8",
                        ) as f:
                            transcripts[original_path] = json.load(f)
                    except Exception as e:
                        print(f"Error loading transcript {filename}: {e}")

        return transcripts

    def _create_app(self) -> FastAPI:
        """Create and configure FastAPI application"""
        app = FastAPI(
            title="AI Multimodal Search API",
            description="""
            üîç **AI-powered multimodal search API**
            
            Search through images and videos using natural language descriptions.
            Powered by OpenAI's CLIP, Whisper, and FAISS.
            
            **Features:**
            - üñºÔ∏è Visual search using text descriptions (for images and video frames)
            - üó£Ô∏è Audio search in video transcripts
            - üöÄ Unified search combining visual and audio modalities
            """,
            version="1.1.0",  # C·∫¨P NH·∫¨T: TƒÉng phi√™n b·∫£n
        )

        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

        self._add_routes(app)
        return app

    def _add_routes(self, app: FastAPI):
        """Add API routes to FastAPI app"""

        # Pydantic models
        class SearchQuery(BaseModel):
            text: str = Field(
                ...,
                description="Natural language description",
                example="red car on street",
            )
            top_k: int = Field(
                default=10, ge=1, le=50, description="Number of results (1-50)"
            )

        class VisualResult(BaseModel):
            id: str = Field(description="Image/Video identifier")
            score: float = Field(description="Similarity score")
            timestamp: Optional[float] = Field(
                description="Timestamp in video if applicable"
            )
            image: Optional[str] = Field(description="Base64-encoded image data")

        class AudioResult(BaseModel):
            id: str = Field(description="Video/Audio identifier")
            score: float = Field(description="Match score")
            match: str = Field(description="The matched text segment")
            start: float = Field(description="Start time of the segment")
            end: float = Field(description="End time of the segment")

        class UnifiedResult(BaseModel):
            id: str = Field(description="Video/Media identifier")
            score: float = Field(description="Final fused score")
            reason: List[str] = Field(
                description="Reasons for the match (visual, audio)"
            )
            image: Optional[str] = Field(description="Base64-encoded thumbnail")

        class VisualSearchResponse(BaseModel):
            results: List[VisualResult]

        class AudioSearchResponse(BaseModel):
            results: List[AudioResult]

        class UnifiedSearchResponse(BaseModel):
            results: List[UnifiedResult]

        # Routes
        @app.get("/")
        def root():
            """API health check and basic info"""
            return {
                "message": "AI Multimodal Search API is running!",
                "status": "ok",
                "version": "1.1.0",
                "indexed_visuals": len(self.index_to_path),
                "indexed_audios": len(self.transcripts),
                "model_info": self.image_processor.get_model_info(),
            }

        # ... (endpoint /stats v√† /image/{filename} gi·ªØ nguy√™n) ...

        @app.post("/search_visual", response_model=VisualSearchResponse)
        def search_visual(query: SearchQuery):
            """Search for images/video frames using natural language"""
            return self._search_visual(query.text, query.top_k)

        # Endpoint t√¨m ki·∫øm audio
        @app.post("/search_audio", response_model=AudioSearchResponse)
        def search_audio(query: SearchQuery):
            """Search for spoken text in video transcripts"""
            return self._search_audio(query.text, query.top_k)

        # Endpoint t√¨m ki·∫øm h·ª£p nh·∫•t
        @app.post("/unified_search", response_model=UnifiedSearchResponse)
        def unified_search(query: SearchQuery):
            """Perform a unified search across visual and audio modalities"""
            print(f"üöÄ Performing unified search for: '{query.text}'")

            # 1. Th·ª±c hi·ªán t√¨m ki·∫øm tr√™n t·ª´ng modality (l·∫•y nhi·ªÅu h∆°n top_k ƒë·ªÉ fusion)
            visual_results_data = self._search_visual(query.text, top_k=50)
            audio_results_data = self._search_audio(query.text, top_k=50)

            # 2. H·ª£p nh·∫•t k·∫øt qu·∫£ b·∫±ng RRF
            fused_scores = self._fuse_results_rrf(
                visual_results_data["results"], audio_results_data["results"]
            )

            # 3. Chu·∫©n b·ªã k·∫øt qu·∫£ cu·ªëi c√πng
            final_results = []
            sorted_media = sorted(
                fused_scores.items(), key=lambda item: item[1]["score"], reverse=True
            )

            for media_id, data in sorted_media[: query.top_k]:
                # L·∫•y ·∫£nh thumbnail cho k·∫øt qu·∫£
                image_base64 = None
                for index, path in self.index_to_path.items():
                    if os.path.basename(path) == media_id:
                        image_base64 = ImageProcessor.image_to_base64(path)
                        break

                final_results.append(
                    UnifiedResult(
                        id=media_id,
                        score=data["score"],
                        reason=data["reason"],
                        image=image_base64,
                    )
                )

            return {"results": final_results}

    # ƒê·ªïi t√™n _search_images th√†nh _search_visual ƒë·ªÉ nh·∫•t qu√°n
    def _search_visual(self, text: str, top_k: int = 10) -> dict:
        """Internal method to search for images/frames"""
        print(f"üîç Visual search for: '{text}' (top_k={top_k})")
        try:
            query_vector = self.image_processor.text_to_embedding(text)
            distances, indices = self.index.search(query_vector, top_k)

            results = []
            for i in range(len(indices[0])):
                result_index = str(indices[0][i])
                # mapping c·ªßa b·∫°n l∆∞u key l√† string {"0": "path"}
                result_path = self.index_to_path.get(result_index)

                if result_path:
                    filename = os.path.basename(result_path)
                    score = float(distances[0][i])

                    # Gi·∫£ ƒë·ªãnh mapping l∆∞u th√¥ng tin timestamp n·∫øu l√† video
                    # Trong low_implementation.md, b·∫°n s·∫Ω c·∫ßn c·∫≠p nh·∫≠t indexer_visual.py
                    # ƒë·ªÉ l∆∞u `{"video_path": path, "timestamp_sec": sec}`
                    # ·ªû ƒë√¢y ta t·∫°m b·ªè qua timestamp cho ƒë∆°n gi·∫£n

                    result_data = {"id": filename, "score": score, "timestamp": None}
                    image_base64 = ImageProcessor.image_to_base64(result_path)
                    if image_base64:
                        result_data["image"] = image_base64
                    results.append(result_data)

            return {"results": results}
        except Exception as e:
            print(f"‚ùå Error in visual search: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    #  H√†m t√¨m ki·∫øm audio
    def _search_audio(self, text: str, top_k: int = 10) -> dict:
        """Internal method to search through transcripts (naive implementation)."""
        print(f"üó£Ô∏è Audio search for: '{text}' (top_k={top_k})")

        # ƒê√¢y l√† c√°ch t√¨m ki·∫øm tu·∫ßn t·ª±, ch·∫≠m v·ªõi d·ªØ li·ªáu l·ªõn.
        # Gi·∫£i ph√°p t·ªët h∆°n l√† d√πng Elasticsearch/OpenSearch.

        found_matches = []
        query_lower = text.lower()

        for media_id, segments in self.transcripts.items():
            for segment in segments:
                if query_lower in segment["text"].lower():
                    # G√°n ƒëi·ªÉm ƒë∆°n gi·∫£n, m·ªói l·∫ßn t√¨m th·∫•y l√† 1 ƒëi·ªÉm
                    found_matches.append(
                        {
                            "id": media_id,
                            "score": 1.0,
                            "match": segment["text"],
                            "start": segment["start"],
                            "end": segment["end"],
                        }
                    )

        # V√¨ c√°ch t√≠nh ƒëi·ªÉm ƒë∆°n gi·∫£n, ta ch·ªâ tr·∫£ v·ªÅ top_k k·∫øt qu·∫£ ƒë·∫ßu ti√™n t√¨m th·∫•y
        return {"results": found_matches[:top_k]}

    # H√†m h·ª£p nh·∫•t k·∫øt qu·∫£
    def _fuse_results_rrf(
        self, visual_results: List, audio_results: List, k: int = 60
    ) -> Dict:
        """Fuse results from different modalities using Reciprocal Rank Fusion (RRF)."""
        fused_scores = {}

        # Process visual results
        for rank, res in enumerate(visual_results):
            media_id = res["id"]
            if media_id not in fused_scores:
                fused_scores[media_id] = {"score": 0.0, "reason": []}

            rrf_score = 1.0 / (k + rank + 1)
            fused_scores[media_id]["score"] += rrf_score
            if "visual" not in fused_scores[media_id]["reason"]:
                fused_scores[media_id]["reason"].append("visual")

        # Process audio results
        for rank, res in enumerate(audio_results):
            media_id = res["id"]
            if media_id not in fused_scores:
                fused_scores[media_id] = {"score": 0.0, "reason": []}

            rrf_score = 1.0 / (k + rank + 1)
            fused_scores[media_id]["score"] += rrf_score
            if "audio" not in fused_scores[media_id]["reason"]:
                fused_scores[media_id]["reason"].append("audio")

        return fused_scores

    def run(self, host: str = "127.0.0.1", port: int = 8000):
        """Run the API server"""
        import uvicorn

        print("üöÄ Starting API server...")
        print(f"Server running at: http://{host}:{port}")
        uvicorn.run(self.app, host=host, port=port)


# Main execution
if __name__ == "__main__":
    # Configuration
    INDEX_FILE = "output/faiss_visual.index"
    MAPPING_FILE = "output/index_to_path.json"
    TRANSCRIPT_DIR = "output/transcripts"
    MODEL_ID = "openai/clip-vit-base-patch32"

    server = APIServer(
        index_file=INDEX_FILE,
        mapping_file=MAPPING_FILE,
        transcript_dir=TRANSCRIPT_DIR,
        model_id=MODEL_ID,
    )
    server.run()
